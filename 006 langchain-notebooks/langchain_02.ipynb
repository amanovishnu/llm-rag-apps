{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -qU -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decouple import AutoConfig, config\n",
    "from icecream import ic\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.documents import Document\n",
    "from langchain.chains import create_retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig(search_path='../notes-rag/')\n",
    "\n",
    "LANGCHAIN_TRACING_V2 = config(\"LANGCHAIN_TRACING_V2\")\n",
    "LANGCHAIN_ENDPOINT = config(\"LANGCHAIN_ENDPOINT\")\n",
    "LANGCHAIN_PROJECT = config(\"LANGCHAIN_PROJECT\")\n",
    "OPENAI_API_KEY = config(\"OPENAI_API_KEY\")\n",
    "lANGCHAIN_API_KEY = config(\"LANGCHAIN_API_KEY\")\n",
    "HF_TOKEN = config(\"HF_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(web_path='https://docs.smith.langchain.com/prompt_engineering/concepts').load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=200)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "split_docs = text_splitter.split_documents(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith\n",
      "-----\n",
      "Skip to main contentLearn the essentials of LangSmith in the new Introduction to LangSmith course!  Enroll for free. API ReferenceRESTPythonSearchRegionUSEUGo to AppQuick StartObservabilityEvaluationPrompt EngineeringConceptual GuideHow-to GuidesPlaygroundPromptsTutorialsOptimize a classifierDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions\n",
      "-----\n",
      "(LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringOn this pageConcepts\n",
      "-----\n",
      "Prompt engineering is one the core pillars of LangSmith.\n",
      "While traditional software application are built by writing code, AI applications often involve a good amount of writing prompts.\n",
      "We aim to make this as easy possible by providing a set of tools designed to enable and facilitate prompt engineering.\n",
      "Why prompt engineering?‚Äã\n",
      "A prompt sets the stage for the model, like an audience member at an improv show directing the actor's next performance - it guides the model's\n",
      "-----\n",
      "Why prompt engineering?‚Äã\n",
      "A prompt sets the stage for the model, like an audience member at an improv show directing the actor's next performance - it guides the model's\n",
      "behavior without changing its underlying capabilities. Just as telling an actor to \"be a pirate\" determines how they act,\n",
      "a prompt provides instructions, examples, and context that shape how the model responds.\n",
      "Prompt engineering is important because it allows you to change the way the model behaves.\n",
      "-----\n",
      "a prompt provides instructions, examples, and context that shape how the model responds.\n",
      "Prompt engineering is important because it allows you to change the way the model behaves.\n",
      "While there are other ways to change the model's behavior (like fine-tuning), prompt engineering is usually the simplest to get started with\n",
      "and often provides the highest ROI.\n",
      "We often see that prompt engineering is multi-disciplinary.\n",
      "-----\n",
      "and often provides the highest ROI.\n",
      "We often see that prompt engineering is multi-disciplinary.\n",
      "Sometimes the best prompt engineer is not the software engineer who is building the application, but rather the product manager\n",
      "or another domain expert.\n",
      "It is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\n",
      "Prompts vs Prompt Templates‚Äã\n",
      "-----\n",
      "or another domain expert.\n",
      "It is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\n",
      "Prompts vs Prompt Templates‚Äã\n",
      "Although we often use these terms interchangably, it is important to understand the difference between \"prompts\" and \"prompt templates\".\n",
      "Prompts refer to the messages that are passed into the language model.\n",
      "-----\n",
      "Prompts refer to the messages that are passed into the language model.\n",
      "Prompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates\n",
      "can include variables for few shot examples, outside context, or any other external data that is needed in your prompt.\n",
      "-----\n",
      "Prompts in LangSmith‚Äã\n",
      "You can store and version prompts templates in LangSmith.\n",
      "There are few key aspects of a prompt template to understand.\n",
      "Chat vs Completion‚Äã\n",
      "There are two different types of prompts: chat style prompts and completion style prompts.\n",
      "Chat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.\n",
      "-----\n",
      "Chat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.\n",
      "Completion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\n",
      "F-string vs mustache‚Äã\n",
      "You can format your prompt with input variables using either f-string or mustache format. Here is an example prompt\n",
      "with f-string format:\n",
      "Hello, {name}!\n",
      "And here is one with mustache:\n",
      "Hello, {{name}}!\n",
      "-----\n",
      "with f-string format:\n",
      "Hello, {name}!\n",
      "And here is one with mustache:\n",
      "Hello, {{name}}!\n",
      "Mustache formatMustache format gives your more flexbility around conditional variables, loops, and nested keys.\n",
      "Read the documentation\n",
      "Tools‚Äã\n",
      "Tools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description,\n",
      "and JSON schema of arguments used to call the tool.\n",
      "Structured Output‚Äã\n",
      "-----\n",
      "Tools‚Äã\n",
      "Tools are interfaces the LLM can use to interact with the outside world. Tools consist of a name, description,\n",
      "and JSON schema of arguments used to call the tool.\n",
      "Structured Output‚Äã\n",
      "Structured output is a feature of most state of the art LLMs, wherein instead of producing raw text as output they\n",
      "stick to a specified schema. This may or may not use Tools under the hood.\n",
      "-----\n",
      "stick to a specified schema. This may or may not use Tools under the hood.\n",
      "Structured Output vs ToolsStructured outputs are similar to tools, but different in a few key ways. With tools, the LLM choose which tool to call (or may choose not to call any); with structured output, the LLM always responds in this format. With tools, the LLM may select multiple tools; with structured output, only one response is generate.\n",
      "Model‚Äã\n",
      "-----\n",
      "Model‚Äã\n",
      "Optionally, you can store a model configuration alongside a prompt template. This includes the name of the model and any other parameters (temperature, etc).\n",
      "Prompt Versioning‚Äã\n",
      "Verisioning is a key part of iterating and collaborating on your different prompts.\n",
      "Commits‚Äã\n",
      "Every time you save a new version of a prompt, it is saved with a new commit.\n",
      "You can view old commit, allowing you to easily see previous prompt versions in case you need to revert to previous functionality.\n",
      "-----\n",
      "You can view old commit, allowing you to easily see previous prompt versions in case you need to revert to previous functionality.\n",
      "You can access a specific commit of the prompt in the SDK by specifying a commit alongside the prompt name.\n",
      "Tags‚Äã\n",
      "You may want to tag prompt commits with a human-readable tag so that you can refer to it even as new commits are added. Common use cases include tagging a prompt with dev or prod tags. This allows you to track which versions of prompts are used where.\n",
      "-----\n",
      "Prompt Playground‚Äã\n",
      "The prompt playground makes the process of iterating and testing your prompts seamless. You can enter the playground from the sidebar or directly from a saved prompt.\n",
      "In the playground you can:\n",
      "-----\n",
      "Change the model being used\n",
      "Change prompt template being used\n",
      "Change the output schema\n",
      "Change the tools available\n",
      "Enter the input variables to run through the prompt template\n",
      "Run the prompt through the model\n",
      "Observe the outputs\n",
      "\n",
      "Testing multiple prompts‚Äã\n",
      "You can add more prompts to your playground to easily compare outputs and decide which version is better:\n",
      "-----\n",
      "Testing multiple prompts‚Äã\n",
      "You can add more prompts to your playground to easily compare outputs and decide which version is better:\n",
      "\n",
      "Testing over a dataset‚Äã\n",
      "To test over a dataset, you simply select the dataset from the top right and press Start. You can modify whether the results\n",
      "are streamed back as well as how many repitions there are in the test.\n",
      "-----\n",
      "You can click on the \"View Experiment\" button to dive deeper into the results of the test.\n",
      "Prompt Canvas‚Äã\n",
      "The prompt canvas makes it easy to edit a prompt with the help of an LLM. This allows you to iterate\n",
      "faster on long prompts and also makes it easier to make overarching stylisting or tonal changes to your prompt.\n",
      "You can enter the promp canvas by clicking the glowing wand over any message in your prompt:\n",
      "-----\n",
      "Chat sidebar‚Äã\n",
      "You can use the chat sidebar to ask questions about your prompt, or to give instructions in natural language to the LLM for how to rewrite your prompt.\n",
      "\n",
      "Write directly‚Äã\n",
      "You can also edit the prompt directly - you don't need to use the LLM. This is useful if you know what edits you want to make and just want to make them directly\n",
      "Quick actions‚Äã\n",
      "There are quick actions to change the reading level or length of the prompt with a single mouse click:\n",
      "-----\n",
      "Custom quick actions‚Äã\n",
      "You can also save your own custom quick actions, for ease of use across all the prompts you are working on in LangSmith:\n",
      "\n",
      "Diffing‚Äã\n",
      "You can also see the specific differences between each version of your prompt by selecting the diff slider in the top right of the canvas:\n",
      "-----\n",
      "Saving and using‚Äã\n",
      "Lastly, you can save the prompt you have created in the canvas by clicking the \"Use this Version\" button in the bottom right:\n",
      "-----\n",
      "Was this page helpful?You can leave detailed feedback on GitHub.PreviousRunning SWE-bench with LangSmithNextConceptual GuideWhy prompt engineering?Prompts vs Prompt TemplatesPrompts in LangSmithChat vs CompletionF-string vs mustacheToolsStructured OutputModelPrompt VersioningCommitsTagsPrompt PlaygroundTesting multiple promptsTesting over a datasetPrompt CanvasChat sidebarWrite directlyQuick actionsCustom quick actionsDiffingSaving and usingCommunityDiscordTwitterGitHubDocs CodeLangSmith\n",
      "-----\n",
      "PlaygroundTesting multiple promptsTesting over a datasetPrompt CanvasChat sidebarWrite directlyQuick actionsCustom quick actionsDiffingSaving and usingCommunityDiscordTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright ¬© 2024 LangChain, Inc.\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for doc in split_docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompts refer to the messages that are passed into the language model.\\nPrompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates\\ncan include variables for few shot examples, outside context, or any other external data that is needed in your prompt.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='or another domain expert.\\nIt is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\\nPrompts vs Prompt Templates\\u200b\\nAlthough we often use these terms interchangably, it is important to understand the difference between \"prompts\" and \"prompt templates\".\\nPrompts refer to the messages that are passed into the language model.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompts in LangSmith\\u200b\\nYou can store and version prompts templates in LangSmith.\\nThere are few key aspects of a prompt template to understand.\\nChat vs Completion\\u200b\\nThere are two different types of prompts: chat style prompts and completion style prompts.\\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.'),\n",
       " Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Chat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.\\nCompletion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\\nF-string vs mustache\\u200b\\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt\\nwith f-string format:\\nHello, {name}!\\nAnd here is one with mustache:\\nHello, {{name}}!')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"difference between prompt and prompt template\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever() # converts a vector store to a retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"\n",
    "        Answer the following question based only on the provided context:\n",
    "            <context>\n",
    "                {context}\n",
    "            </context>\n",
    "        \"\"\"\n",
    "    )\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided context, Langchain is described as a good framework.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chain.invoke({\n",
    "    \"input\": \"difference between prompt and prompt template\",\n",
    "    \"context\": [Document(page_content=\"langchain is a good framework\")]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"difference between prompt and prompt template\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'difference between prompt and prompt template',\n",
       " 'context': [Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompts refer to the messages that are passed into the language model.\\nPrompt Templates refer to a way of formatting information to get that prompt to hold the information that you want. Prompt templates\\ncan include variables for few shot examples, outside context, or any other external data that is needed in your prompt.'),\n",
       "  Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='or another domain expert.\\nIt is important to have the proper tooling and infrastructure to support this cross-disciplinary building.\\nPrompts vs Prompt Templates\\u200b\\nAlthough we often use these terms interchangably, it is important to understand the difference between \"prompts\" and \"prompt templates\".\\nPrompts refer to the messages that are passed into the language model.'),\n",
       "  Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Prompts in LangSmith\\u200b\\nYou can store and version prompts templates in LangSmith.\\nThere are few key aspects of a prompt template to understand.\\nChat vs Completion\\u200b\\nThere are two different types of prompts: chat style prompts and completion style prompts.\\nChat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.'),\n",
       "  Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/concepts', 'title': 'Concepts | ü¶úÔ∏èüõ†Ô∏è LangSmith', 'description': 'Prompt engineering is one the core pillars of LangSmith.', 'language': 'en'}, page_content='Chat style prompts are a list of messages. This is the prompting style supported by most model APIs these days, and so this should generally be preferred.\\nCompletion style prompts are just a string. This is an older style of prompting, and so mostly exists for legacy reasons.\\nF-string vs mustache\\u200b\\nYou can format your prompt with input variables using either f-string or mustache format. Here is an example prompt\\nwith f-string format:\\nHello, {name}!\\nAnd here is one with mustache:\\nHello, {{name}}!')],\n",
       " 'answer': 'What is the difference between \"prompts\" and \"prompt templates\" according to the provided context?\\n\\nPrompts refer to the messages that are passed into the language model, while prompt templates are a way of formatting information to include variables for few-shot examples, outside context, or any other external data needed in the prompt.'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
